# Cluster Monitoring Operator

## Modifications in default setup
Default setup of CMO was modified to achieve:
* monitoring metrics from applications (by default CMO is monitoring only infrastructure components)
* self-service capabilities for alerts
* minimize additional reconfiguration required to migrate from previous Prometheus setup for application teams
* ensure high-availability

Required modifications in CMO image (from playbook in openshift-ansible, release-3.11):

* remove ```- "-namespace=openshift-monitoring"``` flag in CMO configuration. 
It is passed to Prometheus Operator to limit scope of monitoring (in terms of CRDs discovery) to single 
```openshift-monitoring``` namespace. Default monitoring scope in Prometheus Operator is ```NamespaceAll```.
* reconfigure Prometheus setup to accept all existing PrometheusRules, ServiceMonitors (from entire cluster):
```
ruleNamespaceSelector:
  any: true
ruleSelector:
  any: true
serviceMonitorNamespaceSelector:
  any: true 
serviceMonitorSelector:
  any: true
```
* add Service Monitors to read metrics exposed on ```/metrics``` and ```/actuator/prometheus``` endpoints by default
* remove Prometheus resource limits

Additional modifications provided in playbook (not in CMO image itself):
* prometheus storage type in PVC
```
{% if openshift_cluster_monitoring_operator_prometheus_storage_class_enabled | bool %}
          storageClassName: {{ openshift_cluster_monitoring_operator_prometheus_storage_class_name }}
{% endif %}
```
* permissions to scrape metrics, read CRDs from entire cluster
```
- apiVersion: v1
  kind: ClusterRole
  metadata:
    name: prometheus-k8s-apps
  rules:
  - apiGroups: [""]
    resources:
    - nodes
    - services
    - endpoints
    - pods
    - prometheusrules
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
    - configmaps
    verbs: ["get"]
```
* loading alertmanager from template (by default config is hardcoded, what is insufficient for cluster-specific setup)

Recommended extensions in default CMO playbook in openshift-ansible, handling our requirements:

* additional flag to turn on/off limiting monitoring scope to single namespace 
* add possibility to pass configuration of prometheus-k8s (default prometheus cluster) the same way as 
it is done for AlertManager (configuration is useed to create ```alertmanager-main``` secret, 
which is passed to AlertManager stateful-set)
* extend permissions to monitor entire cluster
* add possibility to configure additional service-monitors within setup (optional, they could be added as post-installation step)

## Overwiev
The Cluster Monitoring Operator manages and updates the Prometheus-based monitoring stack deployed on top of OpenShift.

It contains the following components:

* [Prometheus Operator](https://github.com/coreos/prometheus-operator)
* [Prometheus](https://github.com/prometheus/prometheus)
* [Alertmanager](https://github.com/prometheus/alertmanager) cluster for cluster and application level alerting
* [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)
* [node_exporter](https://github.com/prometheus/node_exporter)

The deployed Prometheus Operator is meant to be leveraged by users to easily deploy new Prometheus setup for their application monitoring.
The Prometheus instance (`prometheus-k8s`) is responsible for monitoring and alerting on cluster and OpenShift components. It should not be extended to monitor user applications.
Alertmanager is a cluster-global component for handling alerts generated by all Prometheus instances deployed in that cluster.

Metrics are collected from the following components

* [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)
* [node_exporter](https://github.com/prometheus/node_exporter)
* Kubelets
* API server
* Prometheus (just `prometheus-k8s` for now)
* Alertmanager

**Important:** The Prometheus Operator managed by the Cluster Monitoring Operator will by default only look for `ServiceMonitor` resources in namespaces containing an `openshift.io/cluster-monitoring` label (with any value).

## Contributing new component integrations

The Cluster Monitoring Operator has many builtin `ServiceMonitor` resources which enable discovering the metrics endpoints of a variety of well-known components. Only components that must be created before the cluster monitoring stack belong in this repository, in order to solve the cyclic dependencies of bootstrapping.

To register a new builtin component, make the following changes:

* Add a new `ServiceMonitor` manifest file to [jsonnet/prometheus.jsonnet](jsonnet/prometheus.jsonnet). An example of this can be seen for the OpenShift component "kube-controllers", [here](https://github.com/openshift/cluster-monitoring-operator/blob/01bfe3789117e7074e893251f2f6d31c816db8fb/jsonnet/prometheus.jsonnet#L113-L145).
* Re-generate the go-bindata code, using the `pkg/manifests/bindata.go` make target. This will also create a new file in `assets/prometheus-k8s/` according to the name given in the jsonnet code.
* Add a constant in [pkg/manifests/manifests.go](pkg/manifests/manifests.go) which points to the new manifest file, from `assets/`.
* Add a new `Factory` method in [pkg/manifests/manifests.go](pkg/manifests/manifests.go) which loads the manifest using the new constant.
* Add a step to `PrometheusTask` in [pkg/tasks/prometheus.go](pkg/tasks/prometheus.go) which creates the `ServiceMonitor` using the `Factory` new method.

To add new builtin recording or alerting rules:

* Add a new [Prometheus rules file](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) to [jsonnet/rules.jsonnet](jsonnet/rules.jsonnet).

Run `make pkg/manifests/bindata.go` after you modify the files and make sure to add the modified files to the commit. All rules are automatically created, so no additional code changes are necessary.

## Roadmap

* Monitor etcd
* Adapt Tectonic inherited alerts with OpenShift operational knowledge

## Testing

### End-to-end tests

Run e2e-tests with `make e2e-test`.
Clean up after e2e-tests with `make e2e-clean`
